/*

     FFTE: A FAST FOURIER TRANSFORM PACKAGE

     (C) COPYRIGHT SOFTWARE, 2000-2004, 2008-2014, ALL RIGHTS RESERVED
                BY
         DAISUKE TAKAHASHI
         FACULTY OF ENGINEERING, INFORMATION AND SYSTEMS
         UNIVERSITY OF TSUKUBA
         1-1-1 TENNODAI, TSUKUBA, IBARAKI 305-8573, JAPAN
         E-MAIL: daisuke@cs.tsukuba.ac.jp


     WRITTEN BY DAISUKE TAKAHASHI

     THIS KERNEL WAS GENERATED BY SPIRAL 8.2.0a03
*/

#include <arm_sve.h>

void dft5b_(float64_t  *Y, float64_t  *X, float64_t  *TW1, int  *lp1, int  *mp1) {
    float64_t a408, a409, a410, a411, a412, a413, a414, a415;
    int a406, a407, j1, l1, m1;
    svfloat64x2_t s196, s199, s202, s205, s208, svex2_10, svex2_6, svex2_7, 
            svex2_8, svex2_9;
    svfloat64_t s197, s198, s200, s201, s203, s204, s206, s207, 
            s209, s210, s211, s212, s213, s214, s215, s216, 
            s217, s218, s219, s220, s221, s222, s223, s224, 
            s225, s226, s227, s228, t395, t396, t397, t398, 
            t399, t400, t401, t402, t403, t404, t405, t406, 
            t407, t408, t409, t410, t411, t412, t413, t414, 
            t415, t416, t417, t418;
    svbool_t pg1;
    l1 = *(lp1);
    m1 = *(mp1);
    for(int j2 = 0; j2 < (l1 - 1); j2++) {
        j1 = (j2 + 1);
        int k1 = 0;
        pg1 = svwhilelt_b64(k1, m1);
        do {
            a406 = (k1 + ((j1)*(m1)));
            s196 = svld2_f64(pg1, (X + ((2)*(a406))));
            s197 = s196.v0;
            s198 = s196.v1;
            s199 = svld2_f64(pg1, (X + ((2)*((a406 + ((l1)*(m1)))))));
            s200 = s199.v0;
            s201 = s199.v1;
            s202 = svld2_f64(pg1, (X + ((2)*((a406 + ((((2)*(l1)))*(m1)))))));
            s203 = s202.v0;
            s204 = s202.v1;
            s205 = svld2_f64(pg1, (X + ((2)*((a406 + ((((3)*(l1)))*(m1)))))));
            s206 = s205.v0;
            s207 = s205.v1;
            s208 = svld2_f64(pg1, (X + ((2)*((a406 + ((((4)*(l1)))*(m1)))))));
            s209 = s208.v0;
            s210 = s208.v1;
            t395 = svadd_f64_x(pg1, s200, s209);
            t396 = svadd_f64_x(pg1, s201, s210);
            t397 = svsub_f64_x(pg1, s200, s209);
            t398 = svsub_f64_x(pg1, s201, s210);
            t399 = svadd_f64_x(pg1, s203, s206);
            t400 = svadd_f64_x(pg1, s204, s207);
            t401 = svsub_f64_x(pg1, s203, s206);
            t402 = svsub_f64_x(pg1, s204, s207);
            t403 = svadd_f64_x(pg1, t395, t399);
            t404 = svadd_f64_x(pg1, t396, t400);
            t405 = svadd_f64_x(pg1, t397, t402);
            t406 = svsub_f64_x(pg1, t398, t401);
            t407 = svsub_f64_x(pg1, t397, t402);
            t408 = svadd_f64_x(pg1, t398, t401);
            t409 = svmls_n_f64_x(pg1, s197, t403, 0.25);
            t410 = svmls_n_f64_x(pg1, s198, t404, 0.25);
            s225 = svmla_n_f64_x(pg1, t405, t406, 1.6180339887498947);
            s211 = svmul_n_f64_x(pg1, s225, 0.29389262614623657);
            s226 = svmls_n_f64_x(pg1, t406, t405, 1.6180339887498947);
            s212 = svmul_n_f64_x(pg1, s226, 0.29389262614623657);
            s213 = svmul_n_f64_x(pg1, svsub_f64_x(pg1, t395, t399), 0.55901699437494745);
            s214 = svmul_n_f64_x(pg1, svsub_f64_x(pg1, t396, t400), 0.55901699437494745);
            s227 = svmls_n_f64_x(pg1, t408, t407, 0.61803398874989479);
            s215 = svmul_n_f64_x(pg1, s227, 0.47552825814757682);
            s228 = svmla_n_f64_x(pg1, t407, t408, 0.61803398874989479);
            s216 = svmul_n_f64_x(pg1, s228, 0.47552825814757682);
            t411 = svadd_f64_x(pg1, t409, s213);
            t412 = svadd_f64_x(pg1, t410, s214);
            t413 = svsub_f64_x(pg1, t409, s213);
            t414 = svsub_f64_x(pg1, t410, s214);
            t415 = svadd_f64_x(pg1, s211, s215);
            t416 = svsub_f64_x(pg1, s212, s216);
            t417 = svsub_f64_x(pg1, s211, s215);
            t418 = svadd_f64_x(pg1, s212, s216);
            s217 = svadd_f64_x(pg1, t411, t415);
            s218 = svadd_f64_x(pg1, t412, t416);
            s219 = svsub_f64_x(pg1, t411, t415);
            s220 = svsub_f64_x(pg1, t412, t416);
            s221 = svadd_f64_x(pg1, t413, t418);
            s222 = svsub_f64_x(pg1, t414, t417);
            s223 = svsub_f64_x(pg1, t413, t418);
            s224 = svadd_f64_x(pg1, t414, t417);
            a407 = ((8)*(j1));
            a408 = TW1[a407];
            a409 = TW1[(a407 + 1)];
            a410 = TW1[(a407 + 2)];
            a411 = TW1[(a407 + 3)];
            a412 = TW1[(a407 + 4)];
            a413 = TW1[(a407 + 5)];
            a414 = TW1[(a407 + 6)];
            a415 = TW1[(a407 + 7)];
            svex2_6.v0 = svadd_f64_x(pg1, s197, t403);
            svex2_6.v1 = svadd_f64_x(pg1, s198, t404);
            svst2_f64(pg1, (Y + ((2)*((k1 + ((((5)*(j1)))*(m1)))))), svex2_6);
            svex2_7.v0 = svnmls_n_f64_x(pg1, svmul_n_f64_x(pg1, s218, a409), s217, a408);
            svex2_7.v1 = svmla_n_f64_x(pg1, svmul_n_f64_x(pg1, s218, a408), s217, a409);
            svst2_f64(pg1, (Y + ((2)*((k1 + ((((5)*(j1)))*(m1)) + m1)))), svex2_7);
            svex2_8.v0 = svnmls_n_f64_x(pg1, svmul_n_f64_x(pg1, s222, a411), s221, a410);
            svex2_8.v1 = svmla_n_f64_x(pg1, svmul_n_f64_x(pg1, s222, a410), s221, a411);
            svst2_f64(pg1, (Y + ((2)*((k1 + ((((5)*(j1)))*(m1)) + ((2)*(m1)))))), svex2_8);
            svex2_9.v0 = svnmls_n_f64_x(pg1, svmul_n_f64_x(pg1, s224, a413), s223, a412);
            svex2_9.v1 = svmla_n_f64_x(pg1, svmul_n_f64_x(pg1, s224, a412), s223, a413);
            svst2_f64(pg1, (Y + ((2)*((k1 + ((((5)*(j1)))*(m1)) + ((3)*(m1)))))), svex2_9);
            svex2_10.v0 = svnmls_n_f64_x(pg1, svmul_n_f64_x(pg1, s220, a415), s219, a414);
            svex2_10.v1 = svmla_n_f64_x(pg1, svmul_n_f64_x(pg1, s220, a414), s219, a415);
            svst2_f64(pg1, (Y + ((2)*((k1 + ((((5)*(j1)))*(m1)) + ((4)*(m1)))))), svex2_10);
            k1 += svcntd();
            pg1 = svwhilelt_b64(k1, m1);
        } while(svptest_any(svptrue_b64(), pg1));
    }
}

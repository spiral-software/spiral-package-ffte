/*

     FFTE: A FAST FOURIER TRANSFORM PACKAGE

     (C) COPYRIGHT SOFTWARE, 2000-2004, 2008-2014, ALL RIGHTS RESERVED
                BY
         DAISUKE TAKAHASHI
         FACULTY OF ENGINEERING, INFORMATION AND SYSTEMS
         UNIVERSITY OF TSUKUBA
         1-1-1 TENNODAI, TSUKUBA, IBARAKI 305-8573, JAPAN
         E-MAIL: daisuke@cs.tsukuba.ac.jp


     WRITTEN BY DAISUKE TAKAHASHI

     THIS KERNEL WAS GENERATED BY SPIRAL 8.2.0a03
*/

#include <arm_sve.h>

void dft6c_(float64_t  *Y, float64_t  *X, int  *lp1, int  *mp1) {
    int l1, m1;
    svfloat64x2_t s247, s250, s253, s256, s259, s262, svex2_10, svex2_11, 
            svex2_12, svex2_7, svex2_8, svex2_9;
    svfloat64_t s248, s249, s251, s252, s254, s255, s257, s258, 
            s260, s261, s263, s264, s265, s266, s267, s268, 
            s269, s270, s271, s272, s273, s274, s275, s276, 
            t494, t495, t496, t497, t498, t499, t500, t501, 
            t502, t503, t504, t505, t506, t507, t508, t509, 
            t510, t511, t512, t513;
    svbool_t pg1;
    l1 = *(lp1);
    m1 = *(mp1);
    int k1 = 0;
    pg1 = svwhilelt_b64(k1, m1);
    do {
        s247 = svld2_f64(pg1, (X + ((2)*(k1))));
        s248 = s247.v0;
        s249 = s247.v1;
        s250 = svld2_f64(pg1, (X + ((2)*((k1 + ((l1)*(m1)))))));
        s251 = s250.v0;
        s252 = s250.v1;
        s253 = svld2_f64(pg1, (X + ((2)*((k1 + ((((2)*(l1)))*(m1)))))));
        s254 = s253.v0;
        s255 = s253.v1;
        s256 = svld2_f64(pg1, (X + ((2)*((k1 + ((((3)*(l1)))*(m1)))))));
        s257 = s256.v0;
        s258 = s256.v1;
        s259 = svld2_f64(pg1, (X + ((2)*((k1 + ((((4)*(l1)))*(m1)))))));
        s260 = s259.v0;
        s261 = s259.v1;
        s262 = svld2_f64(pg1, (X + ((2)*((k1 + ((((5)*(l1)))*(m1)))))));
        s263 = s262.v0;
        s264 = s262.v1;
        t494 = svadd_f64_x(pg1, s254, s260);
        t495 = svadd_f64_x(pg1, s255, s261);
        t496 = svadd_f64_x(pg1, s248, t494);
        t497 = svadd_f64_x(pg1, s249, t495);
        t498 = svmls_n_f64_x(pg1, s248, t494, 0.5);
        t499 = svmls_n_f64_x(pg1, s249, t495, 0.5);
        s265 = svmul_n_f64_x(pg1, svsub_f64_x(pg1, s255, s261), 0.8660254037844386);
        s266 = svmul_n_f64_x(pg1, svsub_f64_x(pg1, s254, s260), 0.8660254037844386);
        t500 = svadd_f64_x(pg1, t498, s265);
        t501 = svsub_f64_x(pg1, t499, s266);
        t502 = svsub_f64_x(pg1, t498, s265);
        t503 = svadd_f64_x(pg1, t499, s266);
        t504 = svadd_f64_x(pg1, s257, s263);
        t505 = svadd_f64_x(pg1, s258, s264);
        t506 = svadd_f64_x(pg1, s251, t504);
        t507 = svadd_f64_x(pg1, s252, t505);
        t508 = svmls_n_f64_x(pg1, s251, t504, 0.5);
        t509 = svmls_n_f64_x(pg1, s252, t505, 0.5);
        s267 = svmul_n_f64_x(pg1, svsub_f64_x(pg1, s258, s264), 0.8660254037844386);
        s268 = svmul_n_f64_x(pg1, svsub_f64_x(pg1, s257, s263), 0.8660254037844386);
        t510 = svadd_f64_x(pg1, t508, s267);
        t511 = svsub_f64_x(pg1, t509, s268);
        t512 = svsub_f64_x(pg1, t508, s267);
        t513 = svadd_f64_x(pg1, t509, s268);
        s273 = svmla_n_f64_x(pg1, t510, t511, 1.7320508075688772);
        s269 = svmul_n_f64_x(pg1, s273, 0.5);
        s274 = svmls_n_f64_x(pg1, t511, t510, 1.7320508075688772);
        s270 = svmul_n_f64_x(pg1, s274, 0.5);
        s275 = svmls_n_f64_x(pg1, t513, t512, 0.57735026918962584);
        s271 = svmul_n_f64_x(pg1, s275, 0.8660254037844386);
        s276 = svmla_n_f64_x(pg1, t512, t513, 0.57735026918962584);
        s272 = svmul_n_f64_x(pg1, s276, 0.8660254037844386);
        svex2_7.v0 = svadd_f64_x(pg1, t496, t506);
        svex2_7.v1 = svadd_f64_x(pg1, t497, t507);
        svst2_f64(pg1, (Y + ((2)*(k1))), svex2_7);
        svex2_8.v0 = svadd_f64_x(pg1, t500, s269);
        svex2_8.v1 = svadd_f64_x(pg1, t501, s270);
        svst2_f64(pg1, (Y + ((2)*((k1 + m1)))), svex2_8);
        svex2_9.v0 = svadd_f64_x(pg1, t502, s271);
        svex2_9.v1 = svsub_f64_x(pg1, t503, s272);
        svst2_f64(pg1, (Y + ((2)*((k1 + ((2)*(m1)))))), svex2_9);
        svex2_10.v0 = svsub_f64_x(pg1, t496, t506);
        svex2_10.v1 = svsub_f64_x(pg1, t497, t507);
        svst2_f64(pg1, (Y + ((2)*((k1 + ((3)*(m1)))))), svex2_10);
        svex2_11.v0 = svsub_f64_x(pg1, t500, s269);
        svex2_11.v1 = svsub_f64_x(pg1, t501, s270);
        svst2_f64(pg1, (Y + ((2)*((k1 + ((4)*(m1)))))), svex2_11);
        svex2_12.v0 = svsub_f64_x(pg1, t502, s271);
        svex2_12.v1 = svadd_f64_x(pg1, t503, s272);
        svst2_f64(pg1, (Y + ((2)*((k1 + ((5)*(m1)))))), svex2_12);
        k1 += svcntd();
        pg1 = svwhilelt_b64(k1, m1);
    } while(svptest_any(svptrue_b64(), pg1));
}

/*

     FFTE: A FAST FOURIER TRANSFORM PACKAGE

     (C) COPYRIGHT SOFTWARE, 2000-2004, 2008-2014, ALL RIGHTS RESERVED
                BY
         DAISUKE TAKAHASHI
         FACULTY OF ENGINEERING, INFORMATION AND SYSTEMS
         UNIVERSITY OF TSUKUBA
         1-1-1 TENNODAI, TSUKUBA, IBARAKI 305-8573, JAPAN
         E-MAIL: daisuke@cs.tsukuba.ac.jp


     WRITTEN BY DAISUKE TAKAHASHI

     THIS KERNEL WAS GENERATED BY SPIRAL 8.2.0a03
*/

#include <arm_sve.h>

void dft3b_(float64_t  *Y, float64_t  *X, float64_t  *TW1, int  *lp1, int  *mp1) {
    float64_t a201, a202, a203, a204;
    int a199, a200, j1, l1, m1;
    svfloat64x2_t s101, s104, s98, svex2_4, svex2_5, svex2_6;
    svfloat64_t s100, s102, s103, s105, s106, s107, s108, s109, 
            s110, s111, s112, s99, t126, t127, t128, t129;
    svbool_t pg1;
    l1 = *(lp1);
    m1 = *(mp1);
    for(int j2 = 0; j2 < (l1 - 1); j2++) {
        j1 = (j2 + 1);
        int k1 = 0;
        pg1 = svwhilelt_b64(k1, m1);
        do {
            a199 = (k1 + ((j1)*(m1)));
            s98 = svld2_f64(pg1, (X + ((2)*(a199))));
            s99 = s98.v0;
            s100 = s98.v1;
            s101 = svld2_f64(pg1, (X + ((2)*((a199 + ((l1)*(m1)))))));
            s102 = s101.v0;
            s103 = s101.v1;
            s104 = svld2_f64(pg1, (X + ((2)*((a199 + ((((2)*(l1)))*(m1)))))));
            s105 = s104.v0;
            s106 = s104.v1;
            t126 = svadd_f64_x(pg1, s102, s105);
            t127 = svadd_f64_x(pg1, s103, s106);
            t128 = svmls_n_f64_x(pg1, s99, t126, 0.5);
            t129 = svmls_n_f64_x(pg1, s100, t127, 0.5);
            s107 = svmul_n_f64_x(pg1, svsub_f64_x(pg1, s103, s106), 0.8660254037844386);
            s108 = svmul_n_f64_x(pg1, svsub_f64_x(pg1, s102, s105), 0.8660254037844386);
            s109 = svadd_f64_x(pg1, t128, s107);
            s110 = svsub_f64_x(pg1, t129, s108);
            s111 = svsub_f64_x(pg1, t128, s107);
            s112 = svadd_f64_x(pg1, t129, s108);
            a200 = ((4)*(j1));
            a201 = TW1[a200];
            a202 = TW1[(a200 + 1)];
            a203 = TW1[(a200 + 2)];
            a204 = TW1[(a200 + 3)];
            svex2_4.v0 = svadd_f64_x(pg1, s99, t126);
            svex2_4.v1 = svadd_f64_x(pg1, s100, t127);
            svst2_f64(pg1, (Y + ((2)*((k1 + ((((3)*(j1)))*(m1)))))), svex2_4);
            svex2_5.v0 = svnmls_n_f64_x(pg1, svmul_n_f64_x(pg1, s110, a202), s109, a201);
            svex2_5.v1 = svmla_n_f64_x(pg1, svmul_n_f64_x(pg1, s110, a201), s109, a202);
            svst2_f64(pg1, (Y + ((2)*((k1 + ((((3)*(j1)))*(m1)) + m1)))), svex2_5);
            svex2_6.v0 = svnmls_n_f64_x(pg1, svmul_n_f64_x(pg1, s112, a204), s111, a203);
            svex2_6.v1 = svmla_n_f64_x(pg1, svmul_n_f64_x(pg1, s112, a203), s111, a204);
            svst2_f64(pg1, (Y + ((2)*((k1 + ((((3)*(j1)))*(m1)) + ((2)*(m1)))))), svex2_6);
            k1 += svcntd();
            pg1 = svwhilelt_b64(k1, m1);
        } while(svptest_any(svptrue_b64(), pg1));
    }
}
